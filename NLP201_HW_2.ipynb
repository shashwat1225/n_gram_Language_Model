{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "from random import sample\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "STOP_TOKEN = \"<STOP>\"\n",
        "START_TOKEN = \"<START>\""
      ],
      "metadata": {
        "id": "Wiq38r15Iewy"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding the dataset\n",
        "def load_data(filename):\n",
        "  text = []\n",
        "  with open(filename, \"r\") as f:\n",
        "    text = [sent for sent in f]\n",
        "  return text\n",
        "#Tokenizing the data by splitting words on space (\" \")\n",
        "def tokenize(raw_corpus):\n",
        "  return [[START_TOKEN] + sent.split() + [STOP_TOKEN] for sent in raw_corpus]\n",
        "#Getting vocab\n",
        "def get_vocab_freq(corpus):\n",
        "  counter = Counter([token for sent in corpus for token in sent])\n",
        "  return dict(counter)\n",
        "#Creating <UNK> tokens for words with low frequency\n",
        "def get_corpus_with_unk(corpus, vocab_freq, freq_constraint=3):\n",
        "  return [[(token if not unknown(vocab_freq[token], token, freq_constraint) else UNK_TOKEN) for token in sent] for sent in corpus]\n",
        "#Word checker for <UNK>\n",
        "def unknown(freq, token, freq_constraint):\n",
        "  return (token is not START_TOKEN and token is not STOP_TOKEN and freq < freq_constraint)"
      ],
      "metadata": {
        "id": "ZHcyW_d3IP_x"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unigram:\n",
        "    def __init__(self):\n",
        "        self.vocab_freq = {}\n",
        "        self.vocab_prob = {}\n",
        "        self.smoothing = None\n",
        "        self.l1 = 1\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab_freq.keys()) - 1\n",
        "\n",
        "    @property\n",
        "    def total_words_num(self):\n",
        "        # excluding the <START> token\n",
        "        return sum(self.vocab_freq.values()) - self.vocab_freq.get(\n",
        "            START_TOKEN, 0\n",
        "        )\n",
        "\n",
        "    def perplexity(self, corpus):\n",
        "        m = sum([len(sent) - 1 for sent in corpus])  # ignore the <START> token\n",
        "        entropy = 0\n",
        "        for sent in corpus:\n",
        "            probs = [\n",
        "                self.vocab_prob[self.convert_unk(word)]\n",
        "                for word in sent\n",
        "                if word is not START_TOKEN\n",
        "            ]\n",
        "            entropy += self.log_joint_probs(probs)\n",
        "        l = entropy / m\n",
        "        return pow(2, -l)\n",
        "\n",
        "    # for calculating log sum of joint probability and handle unseen words (in Unigram, just considering it as unknown)\n",
        "    def log_joint_probs(self, probs):\n",
        "        log_probs = []\n",
        "        for prob in probs:\n",
        "            log_probs.append(math.log(prob if prob else self.handle_zero_prob(), 2))\n",
        "        return sum(log_probs)\n",
        "\n",
        "    def handle_zero_prob(self):\n",
        "        return pow(10, -10)\n",
        "\n",
        "    def train_unigram(self, train_corpus, smoothing=None, l1=1):\n",
        "        self.smoothing = smoothing\n",
        "        self.l1 = l1\n",
        "\n",
        "        self.vocab_freq = get_vocab_freq(train_corpus)\n",
        "        for token in self.vocab_freq.keys():\n",
        "            # ignore start token\n",
        "            if token in [START_TOKEN]:\n",
        "                continue\n",
        "            # cache the probability for all unigram\n",
        "            self.vocab_prob[token] = self.MLE(token, smoothing, l1)\n",
        "\n",
        "    def MLE(self, word, smoothing=None, l1=1):\n",
        "        # three different way for estimating Maximum Likelihood\n",
        "        if smoothing:  \n",
        "            return self.word_prob_with_interpolation(word, l1)\n",
        "        else:\n",
        "            return self.word_prob(word)\n",
        "\n",
        "    # convert word to UNK token if not in the vocabulary\n",
        "    def convert_unk(self, word):\n",
        "        if word is not START_TOKEN and word not in self.vocab_prob.keys():\n",
        "            return UNK_TOKEN\n",
        "        return word\n",
        "\n",
        "    # without any smoothing\n",
        "    def word_prob(self, word):\n",
        "        return self.vocab_freq[word] / self.total_words_num"
      ],
      "metadata": {
        "id": "KMqUz7U6cPb5"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bigram:\n",
        "  def __init__(self):\n",
        "    # for storing pre-trained unigram\n",
        "    self.unigram = None\n",
        "    # frequency count for bigram only\n",
        "    self.vocab_freq = {}\n",
        "    # probability cache for bigram only\n",
        "    self.vocab_prob = {}\n",
        "    # hyperparameters\n",
        "    self.smoothing = None\n",
        "    self.l1 = 0\n",
        "    self.l2 = 1\n",
        "  # set the pre-trained unigram\n",
        "  def set_unigram(self, unigram):\n",
        "    self.unigram = unigram\n",
        "  def perplexity(self, corpus):\n",
        "    m = sum([len(sent) - 1 for sent in corpus])  # ignore the <START> token\n",
        "    \n",
        "    processed_corpus = [[self.unigram.convert_unk(word) for word in sent] for sent in corpus] #For <UNK>\n",
        "    entropy = 0\n",
        "    for sent in processed_corpus:\n",
        "      # if not exist(unseen bigram), just give 0, and we will handle it in the log_joint_probs function\n",
        "      probs = [self.vocab_prob.get(bigram, 0) for bigram in self.get_bigrams(sent)]\n",
        "      entropy += self.log_joint_probs(probs)\n",
        "    l = entropy / m\n",
        "    return pow(2, -l)\n",
        "  def log_joint_probs(self, probs):\n",
        "    log_probs = []\n",
        "    for prob in probs:\n",
        "      log_probs.append(math.log(prob if prob else self.handle_zero_prob(), 2))\n",
        "    return sum(log_probs)\n",
        "  def handle_zero_prob(self):\n",
        "    return pow(10, -10)\n",
        "  def train_bigram(self, train_corpus, smoothing=None, l1=0, l2=1):\n",
        "      self.smoothing = smoothing\n",
        "      self.l1 = l1\n",
        "      self.l2 = l2\n",
        "      self.vocab_freq = get_vocab_freq([self.get_bigrams(sent) for sent in train_corpus])\n",
        "      for bigram in self.vocab_freq.keys():\n",
        "        self.vocab_prob[bigram] = self.MLE(bigram, smoothing, l1, l2)\n",
        "  def MLE(self, bigram, smoothing=None, l1=0, l2=1):\n",
        "    if smoothing : \n",
        "      return self.bigram_prob_with_interpolation(bigram, l1, l2)\n",
        "    else:\n",
        "      return self.bigram_prob(bigram)\n",
        "  def get_bigrams(self, sent):\n",
        "    # get all bigrams in given sentence\n",
        "    return [(sent[i - 1], sent[i]) for i in range(1, len(sent))]\n",
        "  def vocab_size(self):\n",
        "    # excluding the <START> token\n",
        "    return len(self.unigram.vocab_freq.keys()) - 1\n",
        "  def bigram_prob(self, bigram):\n",
        "    return (self.vocab_freq.get(bigram, 0) / self.unigram.vocab_freq[bigram[0]])\n",
        "  def bigram_prob_interpolation(self, bigram, l1=0, l2=1):\n",
        "    return l2 * self.bigram_prob(bigram) + l1 * self.unigram.word_prob(bigram[1])"
      ],
      "metadata": {
        "id": "GNZgqSErDytn"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Trigram:\n",
        "  def __init__(self):\n",
        "    self.bigram = None\n",
        "    self.vocab_freq = {}\n",
        "    self.vocab_prob = {}\n",
        "    self.smoothing = None\n",
        "    self.l1 = 0\n",
        "    self.l2 = 0\n",
        "    self.l3 = 1\n",
        "\n",
        "  def set_bigram(self, bigram):\n",
        "    self.bigram = bigram\n",
        "\n",
        "  def perplexity(self, corpus):\n",
        "    m = sum([len(sent) - 1 for sent in corpus])  # ignore the <START> token\n",
        "    processed_corpus = [[self.bigram.unigram.convert_unk(word) for word in sent] for sent in corpus]\n",
        "    entropy = 0\n",
        "    for sent in processed_corpus:\n",
        "      entropy += self.log_joint_probs(self.MLE_log_with_sentence(sent))\n",
        "    l = entropy / m\n",
        "    return pow(2, -l)\n",
        "\n",
        "  def log_joint_probs(self, probs):\n",
        "    log_probs = []\n",
        "    for prob in probs:\n",
        "      log_probs.append(math.log(prob if prob else self.handle_zero_prob(), 2))\n",
        "    return sum(log_probs)\n",
        "\n",
        "  def handle_zero_prob(self):\n",
        "    return pow(10, -10)\n",
        "\n",
        "  def train_trigram(self, train_corpus, smoothing=False, l1=0, l2=0, l3=1):\n",
        "    self.vocab_freq = get_vocab_freq([self.get_trigrams(sent) for sent in train_corpus])\n",
        "    self.smoothing = smoothing\n",
        "    self.l1 = l1\n",
        "    self.l2 = l2\n",
        "    self.l3 = l3\n",
        "\n",
        "    for trigram in self.vocab_freq.keys():\n",
        "      self.vocab_prob[trigram] = self.MLE(trigram)\n",
        "\n",
        "  def MLE_log_with_sentence(self, sent):\n",
        "    trigrams = self.get_trigrams(sent)\n",
        "    first_trigram = trigrams[0]\n",
        "    # need to calculate the P(w1 | <START>)\n",
        "    w1_start_bigram = (first_trigram[0], first_trigram[1])\n",
        "    bigram_prob = 1\n",
        "    if self.smoothing: \n",
        "      bigram_prob = self.bigram_prob_interpolation(w1_start_bigram, self.l1, self.l2, self.l3)\n",
        "    else:\n",
        "      bigram_prob = self.bigram.bigram_prob(w1_start_bigram)\n",
        "    probs = [bigram_prob]\n",
        "    for trigram in trigrams:\n",
        "      prob = self.vocab_prob.get(trigram, self.MLE(trigram))\n",
        "      probs.append(prob)\n",
        "    return probs\n",
        "\n",
        "  def MLE(self, trigram):\n",
        "    if self.smoothing: \n",
        "      return self.trigram_prob_interpolation(trigram, self.l1, self.l2, self.l3)\n",
        "    else:\n",
        "      return self.trigram_prob(trigram)\n",
        "\n",
        "  def get_trigrams(self, sent):\n",
        "    return [(sent[i - 2], sent[i - 1], sent[i]) for i in range(2, len(sent))]\n",
        "  def vocab_size(self):\n",
        "    return len(self.bigram.unigram.vocab_freq.keys()) - 1\n",
        "\n",
        "  def trigram_prob(self, trigram):\n",
        "    if (trigram[0], trigram[1]) not in self.bigram.vocab_freq.keys():\n",
        "      return pow(10, -10)\n",
        "    return (self.vocab_freq.get(trigram, 0)/self.bigram.vocab_freq[(trigram[0], trigram[1])])\n",
        "  def bigram_prob_interpolation(self, bigram, l1=0, l2=0, l3=1):\n",
        "    return self.bigram.bigram_prob_interpolation(bigram, l1, l2 + l3)\n",
        "\n",
        "  def trigram_prob_interpolation(self, trigram, l1=0, l2=0, l3=1):\n",
        "    return (l3 * self.trigram_prob(trigram) + l2 * self.bigram.vocab_prob.get((trigram[1], trigram[2]), 0) + l1 * self.bigram.unigram.vocab_prob.get(trigram[2], 0))"
      ],
      "metadata": {
        "id": "7mfRgsGcIT6S"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_regular(corpus, n_gram=1):\n",
        "  unigram = Unigram()\n",
        "  unigram.train_unigram(corpus)\n",
        "  if n_gram == 1:\n",
        "      return unigram\n",
        "  bigram = Bigram()\n",
        "  bigram.set_unigram(unigram)\n",
        "  bigram.train_bigram(corpus)\n",
        "  if n_gram == 2:\n",
        "    return bigram\n",
        "  trigram = Trigram()\n",
        "  trigram.set_bigram(bigram)\n",
        "  trigram.train_trigram(corpus)\n",
        "  if n_gram == 3:\n",
        "    return trigram\n",
        "  return None\n",
        "def train_interpolation_trigram(corpus, pretrained_bigram, l1=0.3, l2=0.3, l3=0.4):\n",
        "  trigram_interpolation = Trigram()\n",
        "  trigram_interpolation.set_bigram(pretrained_bigram)\n",
        "  trigram_interpolation.train_trigram(corpus, smoothing=True, l1=l1, l2=l2, l3=l3)\n",
        "  return trigram_interpolation\n",
        "def get_pretrained_bigram(corpus):\n",
        "  return train_regular(corpus, n_gram=2)"
      ],
      "metadata": {
        "id": "hr_pZkwrI3wT"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def debuggin(train_corpus):\n",
        "  unigram = Unigram()\n",
        "  unigram.train_unigram(train_corpus)\n",
        "  print(\"Unigram Perplexity:\", unigram.perplexity(tokenize([\"HDTV .\"])))\n",
        "  bigram = Bigram()\n",
        "  bigram.set_unigram(unigram)\n",
        "  bigram.train_bigram(train_corpus)\n",
        "  print(\"Bigram Perplexity:\", bigram.perplexity(tokenize([\"HDTV .\"])))\n",
        "  trigram = Trigram()\n",
        "  trigram.set_bigram(bigram)\n",
        "  trigram.train_trigram(train_corpus)\n",
        "  print(\"Trigram Perplexity:\", trigram.perplexity(tokenize([\"HDTV .\"])))"
      ],
      "metadata": {
        "id": "PaSwdWLUP8gZ"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data\n",
        "train = load_data(\"/content/drive/MyDrive/NLP201 Assignments/1b_benchmark.train.tokens\")\n",
        "val = load_data(\"/content/drive/MyDrive/NLP201 Assignments/1b_benchmark.dev.tokens\")\n",
        "test = load_data(\"/content/drive/MyDrive/NLP201 Assignments/1b_benchmark.test.tokens\")\n",
        "# tokenize the corpus\n",
        "train_corpus = tokenize(train)\n",
        "val_corpus = tokenize(val)\n",
        "test_corpus = tokenize(test)\n",
        "# get the vocab/freq map for all tokens\n",
        "vocab_freq_all = get_vocab_freq(train_corpus)\n",
        "# replace low freq words with unknown tokens\n",
        "train_corpus_with_unk = get_corpus_with_unk(train_corpus, vocab_freq_all)"
      ],
      "metadata": {
        "id": "1DCLn47QP0NC"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DEBUGGING\n",
        "debuggin(train_corpus_with_unk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD6RL-OEP9Ql",
        "outputId": "e3fd304a-8ab5-4126-d8e8-24c9220a37e5"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Perplexity: 658.0445066285465\n",
            "Bigram Perplexity: 63.70757362051907\n",
            "Trigram Perplexity: 39.47865107091444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WITHOUT SMOOTHING - Unigram\n",
        "n_gram = 1\n",
        "lm = train_regular(train_corpus_with_unk, n_gram)\n",
        "print(\"train perplexity:\", lm.perplexity(train_corpus))\n",
        "print(\"dev perplexity:\", lm.perplexity(val_corpus))\n",
        "print(\"test perplexity:\", lm.perplexity(test_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dKi8VuiTS0F",
        "outputId": "2f415c3e-067f-434a-9e43-27e57bbb69a2"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train perplexity: 976.5437422200696\n",
            "dev perplexity: 892.246647512294\n",
            "test perplexity: 896.4994914343403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WITHOUT SMOOTHING - Bigram\n",
        "n_gram = 2\n",
        "lm = train_regular(train_corpus_with_unk, n_gram)\n",
        "print(\"train perplexity:\", lm.perplexity(train_corpus))\n",
        "print(\"dev perplexity:\", lm.perplexity(val_corpus))\n",
        "print(\"test perplexity:\", lm.perplexity(test_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RquyFhdHW7vO",
        "outputId": "c958f691-8954-4a9d-daac-78245bc10189"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train perplexity: 77.07346595628817\n",
            "dev perplexity: 3443.040535106584\n",
            "test perplexity: 3436.4880501475827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WITHOUT SMOOTHING - Trigram\n",
        "n_gram = 3\n",
        "lm = train_regular(train_corpus_with_unk, n_gram)\n",
        "print(\"train perplexity:\", lm.perplexity(train_corpus))\n",
        "print(\"dev perplexity:\", lm.perplexity(val_corpus))\n",
        "print(\"test perplexity:\", lm.perplexity(test_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HeSuyttXNpW",
        "outputId": "02ec2376-f86a-4832-d32e-56629a2fe623"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train perplexity: 7.872967947053928\n",
            "dev perplexity: 2607451.9296221696\n",
            "test perplexity: 2543381.813158857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEBUGGING FOR INTERPOLATION\n",
        "l1 = 0.1\n",
        "l2 = 0.3\n",
        "l3 = 0.6\n",
        "lm = train_interpolation_trigram(train_corpus_with_unk,get_pretrained_bigram(train_corpus_with_unk),l1,l2,l3)\n",
        "print(\"debug perplexity:\", lm.perplexity(tokenize([\"HDTV .\"])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtRe2XlwphMN",
        "outputId": "e9b19810-e451-48df-846e-544724699c12"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debug perplexity: 48.11351783080276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WITH SMOOTHING - Interpolation\n",
        "lm = train_interpolation_trigram(train_corpus_with_unk,get_pretrained_bigram(train_corpus_with_unk),\n",
        "     l1 = 0.33,l2 = 0.33,l3 = 0.34) #Change hyperparameters here for experimentation\n",
        "print(\"train perplexity:\", lm.perplexity(train_corpus))\n",
        "print(\"dev perplexity:\", lm.perplexity(val_corpus))\n",
        "print(\"test perplexity:\", lm.perplexity(test_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPtkNwhFqUaT",
        "outputId": "60070ed8-7d67-4021-d73b-50afd6396943"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train perplexity: 17.039448240154936\n",
            "dev perplexity: 278.1579470723915\n",
            "test perplexity: 277.80247277189716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment with half training corpus\n",
        "experimental_data = sample(train_corpus, int(len(train_corpus) * 0.5))\n",
        "vocab_freq_portion = get_vocab_freq(experimental_data)\n",
        "# replace low freq (according to the defined restriction) words with unknown tokens\n",
        "exp_data_with_unk = get_corpus_with_unk(experimental_data, vocab_freq_portion)\n",
        "\n",
        "pretrained_bigram = get_pretrained_bigram(exp_data_with_unk)\n",
        "lm = train_interpolation_trigram(exp_data_with_unk, pretrained_bigram, l1 = 0.33, l2 = 0.33, l3 = 0.34)    \n",
        "print(\"train perplexity:\", lm.perplexity(train_corpus))\n",
        "print(\"dev perplexity:\", lm.perplexity(val_corpus))\n",
        "print(\"test perplexity:\", lm.perplexity(test_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWc0biTFSixt",
        "outputId": "5b21d6ec-5dbe-4e8e-b3e9-caea418944ca"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train perplexity: 61.35715543047421\n",
            "dev perplexity: 266.21262130921315\n",
            "test perplexity: 264.43533561665595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment with freq_contraint = 5\n",
        "train_corpus_with_unk = get_corpus_with_unk(train_corpus, vocab_freq_all, freq_constraint = 5)\n",
        "lm = train_interpolation_trigram(train_corpus_with_unk,get_pretrained_bigram(train_corpus_with_unk),l1 = 0.33,l2 = 0.33,l3 = 0.34)\n",
        "print(\"train perplexity:\", lm.perplexity(train_corpus))\n",
        "print(\"dev perplexity:\", lm.perplexity(val_corpus))\n",
        "print(\"test perplexity:\", lm.perplexity(test_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC6cXNMXeCbW",
        "outputId": "8dc9fdac-a0d9-452a-e5e5-6eb8d749e7eb"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train perplexity: 18.206104300678007\n",
            "dev perplexity: 233.65011066775767\n",
            "test perplexity: 233.38109644284143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LLDbQ_bsyre-"
      },
      "execution_count": 161,
      "outputs": []
    }
  ]
}